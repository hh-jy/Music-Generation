{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP8LJOducwNDsEZonE8tY5q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Datasets Fusion"],"metadata":{"id":"5kEifHxFJW7d"}},{"cell_type":"markdown","source":["Firstly, organize two CSV files.\n","\n","1. First read the CSV files of each of the datasets.\n","2. Extract important feature columns.\n","3. Organize the CSV file with the file name as a column and the comments as a column.\n","4. Save as a new CSV file."],"metadata":{"id":"-jhVffZrJbAp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2f400f1e"},"outputs":[],"source":["import pandas as pd\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import string\n","\n","from nltk.corpus import sentiwordnet as swn\n","from nltk.corpus import opinion_lexicon\n","from nltk.tag import pos_tag\n","\n","from gensim import corpora\n","from gensim.models import LdaModel"]},{"cell_type":"markdown","metadata":{"id":"16f3f811"},"source":["ArtEmis Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9d3a5966"},"outputs":[],"source":["artwork_data = pd.read_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\artemis_official_data\\\\merged_dataset2.csv\")\n","\n","artwork_data['integrated_comment'] = artwork_data.apply(lambda x: ['art style: ' + x.loc['art_style'], f\"emotion: {x.loc['emotion']}\", x.loc['utterance_x'], x.loc['utterance_y']] if pd.notnull(x['utterance_y']) else ['art style: ' + x.loc['art_style'], f\"emotion: {x.loc['emotion']}\", x.loc['utterance_x']], axis=1)\n","\n","artwork_data[['painting','integrated_comment']].to_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\artemis_official_data\\\\artwork_data.csv\", index=False)"]},{"cell_type":"markdown","source":["Extract emotional words in a new column and then save as a new CSV file."],"metadata":{"id":"2tJ5scEEeOfP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"28b9ff2e","outputId":"215bc58f-81b0-4af8-9d1b-d668cee286e8"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     D:\\downloadSoftware\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package opinion_lexicon to\n","[nltk_data]     D:\\downloadSoftware\\nltk_data...\n","[nltk_data]   Package opinion_lexicon is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     D:\\downloadSoftware\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     D:\\downloadSoftware\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# load stop words\n","nltk.download('stopwords')\n","# sentiment analysis\n","nltk.download('opinion_lexicon')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fc992c1"},"outputs":[],"source":["# pre-process text\n","def text_preprocess(text):\n","    text = text.lower()\n","    # tokenization\n","    tokens = word_tokenize(text)\n","    # remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    filtered_tokens = [word for word in tokens if word not in stop_words]\n","    # remove punctuation\n","    filtered_tokens = [word for word in filtered_tokens if word not in string.punctuation]\n","    ## part-of-speech tagging\n","    tagged_tokens = pos_tag(filtered_tokens)\n","    # lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_tokens = []\n","    for word, tag in tagged_tokens:\n","        if tag.startswith('V'):  # Verb\n","            lemmatized_tokens.append(lemmatizer.lemmatize(word, pos='v'))\n","        elif tag.startswith('RB'):  # Adverb\n","            lemmatized_tokens.append(lemmatizer.lemmatize(word, pos='r'))\n","        else:\n","            lemmatized_tokens.append(word)\n","\n","    return lemmatized_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"640b9ead"},"outputs":[],"source":["# sentiment analysis\n","# load AFINN sentiment dictionary\n","afinn = opinion_lexicon.words()\n","def extract_emotional_words(tokens):\n","    emotional_words = set()\n","    # tag the word part of speech\n","    tagged_tokens = pos_tag(tokens)\n","\n","    for token, tag in tagged_tokens:\n","        if tag.startswith('JJ') or tag.startswith('RB') or tag.startswith('NN') or tag.startswith('VB'):\n","            if token in afinn:\n","                emotional_words.add(token)\n","\n","    return emotional_words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c493395c"},"outputs":[],"source":["artwork_text = artwork_data['utterance']\n","emotional_words_column = []\n","for text in artwork_text:\n","    preprocessed_text = text_preprocess(text)\n","    emotional_words = extract_emotional_words(preprocessed_text)\n","    emotional_words_column.append(\", \".join(emotional_words))\n","\n","artwork_data['emotional_words'] = emotional_words_column\n","\n","artwork_data.to_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\artemis_official_data\\\\processed_artwork_data.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abd170bc"},"outputs":[],"source":["# audio dataset\n","audio_text = audio_data['caption']\n","emotional_words_column = []\n","for text in audio_text:\n","    preprocessed_text = text_preprocess(text)\n","    emotional_words = extract_emotional_words(preprocessed_text)\n","    emotional_words_column.append(\", \".join(emotional_words))\n","\n","audio_data['emotional_words'] = emotional_words_column\n","\n","audio_data.to_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\archive\\\\processed_audio_data.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1e5c3d9"},"outputs":[],"source":["# load the dataset\n","artwork_data = pd.read_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\artemis_official_data\\\\processed_artwork_data.csv\")\n","# audio_data = pd.read_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\archive\\\\processed_audio_data.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9d47694"},"outputs":[],"source":["# 对数据按照画作进行分组\n","grouped_data = artwork_data.groupby('painting')\n","\n","# 存储每幅画作的主题词列表\n","painting_topics = []\n","\n","# 遍历每个分组（每幅画）\n","for name, group in grouped_data:\n","    # 分词处理\n","    texts = group['utterance'].apply(text_preprocess).tolist()\n","\n","    # 构建词典\n","    dictionary = corpora.Dictionary(texts)\n","\n","    # 提取每个评论的词袋表示\n","    corpus = [dictionary.doc2bow(text) for text in texts]\n","\n","    # 训练LDA模型\n","    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, passes=10)\n","\n","    # 提取每个主题的关键词\n","    topic_words = []\n","    for topic_id in range(lda_model.num_topics):\n","        topic_words.extend([word for word, _ in lda_model.show_topic(topic_id)])\n","\n","    # 将主题词列表添加到结果中\n","    painting_topics.append({'painting': name, 'main_topic_words': ', '.join(topic_words)})\n","\n","# 转换为DataFrame并显示在控制台上\n","result_df = pd.DataFrame(painting_topics)\n","result_df.to_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\artemis_official_data\\\\processed_artwork_data2.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"4caa93bd"},"source":["MusicCaps Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2985ac8"},"outputs":[],"source":["audio_data = pd.read_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\archive\\\\updated_musiccaps-public.csv\")\n","\n","audio_data['audio_name'] = audio_data['ytid'] + '_' + audio_data['start_s'].astype(str) + '-' + audio_data['end_s'].astype(str)\n","\n","audio_data[['audio_name', 'caption']].to_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\archive\\\\audio_data.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1353509e"},"outputs":[],"source":["import pandas as pd\n","\n","df1 = pd.read_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\archive\\\\processed_audio_data.csv\")\n","df2 = pd.read_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\archive\\\\updated_musiccaps-public.csv\")\n","\n","column = df2['aspect_list']\n","\n","df1['topic_words'] = column\n","\n","df1.to_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\archive\\\\processed_audio_data2.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"78c85977"},"source":["Use TF-IDF and TINYBERT to fuse two independent datasets based on the content. This concept is inspired by based-content recommendation algorithm.\n","\n","Compare these two methods and choose the best one to achieve datasets fusion."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6c754bd0"},"outputs":[],"source":["# load the dataset\n","artwork_data = pd.read_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\artemis_official_data\\\\processed_artwork_data.csv\")\n","audio_data = pd.read_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\archive\\\\processed_audio_data2.csv\")"]},{"cell_type":"markdown","source":["## TF-IFD"],"metadata":{"id":"bubkUKX0f3Tw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"c412c6c7","outputId":"87e89e44-13fc-476c-c222-54f786f76e9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model Metrics:\n","Coverage: 0.49776216517837696\n","Diversity: 0.008711586748470683\n","Similarity: 0.1324532315258462\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# 构建TF-IDF向量化器\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","# 对艺术画作数据集和音乐数据集的评论进行向量化\n","artwork_comments_matrix = tfidf_vectorizer.fit_transform(artwork_data['utterance'])\n","music_comments_matrix = tfidf_vectorizer.transform(audio_data['caption'])\n","\n","# 定义分批大小\n","batch_size = 1000\n","\n","# 初始化匹配结果列表\n","matched_data = []\n","\n","# 计算相似度矩阵\n","num_artworks = artwork_comments_matrix.shape[0]\n","num_batches = (num_artworks + batch_size - 1) // batch_size\n","\n","for i in range(num_batches):\n","    start_idx = i * batch_size\n","    end_idx = min((i + 1) * batch_size, num_artworks)\n","\n","    # 获取当前批次的艺术画作描述性文本和音乐评论向量化矩阵\n","    artwork_batch = artwork_comments_matrix[start_idx:end_idx]\n","\n","    # 计算当前批次的相似度矩阵\n","    similarity_matrix_batch = cosine_similarity(artwork_batch, music_comments_matrix)\n","\n","    # 获取每幅艺术画作与音乐之间的最匹配评论的索引和相似度得分\n","    best_match_indices = similarity_matrix_batch.argmax(axis=1)\n","    best_match_scores = similarity_matrix_batch.max(axis=1)\n","\n","    # 将匹配结果添加到匹配数据列表中\n","    for j, (artwork_index, score) in enumerate(zip(range(start_idx, end_idx), best_match_scores)):\n","        music_index = best_match_indices[j]\n","        artwork_name = artwork_data.loc[artwork_index, 'painting']\n","        artwork_text = artwork_data.loc[artwork_index, 'utterance']\n","        music_name = audio_data.loc[music_index, 'audio_name']\n","        music_comment = audio_data.loc[music_index, 'caption']\n","        matched_data.append([artwork_name, artwork_text, music_name, music_comment, score])\n","\n","# 将匹配结果整理成新的DataFrame\n","matched_df = pd.DataFrame(matched_data, columns=['Artwork', 'Art_Utterance', 'Music_Name', 'Music_Comment', 'Similarity_Score'])\n","\n","# 将结果保存到新的CSV文件中\n","matched_df.to_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\matched_data_TF_IDF.csv\", index=False)\n","\n","\n","# 计算模型指标\n","# 覆盖率：匹配的艺术画作评论和音乐评论的数量\n","total_artworks_matched = len(matched_df['Artwork'].unique())\n","total_music_matched = len(matched_df['Music_Name'].unique())\n","coverage_artwork = total_artworks_matched / len(artwork_data)\n","coverage_music = total_music_matched / len(audio_data)\n","coverage = (coverage_artwork + coverage_music) / 2\n","\n","# 多样性：匹配的音乐评论的唯一数量\n","diversity = total_music_matched / total_matches if total_matches > 0 else 0\n","\n","# 相似度的均值\n","similarity_mean = matched_df['Similarity_Score'].mean()\n","\n","# 打印模型指标\n","print(\"Model Metrics:\")\n","print(\"Coverage:\", coverage)\n","print(\"Diversity:\", diversity)\n","print(\"Similarity:\", similarity_mean)"]},{"cell_type":"markdown","source":["## TINYBERT"],"metadata":{"id":"_Sb4nmcPgHOR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"86fbab05","outputId":"cd28043f-7068-4963-eeae-3d388d5a8ccb"},"outputs":[{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"name":"stdout","output_type":"stream","text":["Model Metrics:\n","Coverage: 0.20477933710968926\n","Diversity: 0.0024816188900559435\n","Similarity: 0.77331626\n"]}],"source":["import torch\n","from transformers import BertTokenizer, BertModel\n","from sklearn.metrics.pairwise import cosine_similarity\n","import pandas as pd\n","import numpy as np\n","\n","tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-tiny')\n","model = BertModel.from_pretrained('prajjwal1/bert-tiny')\n","\n","# 定义批量处理大小\n","batch_size = 500\n","\n","# 初始化匹配结果列表\n","matched_data = []\n","\n","# 计算批次数量\n","num_artworks = len(artwork_data)\n","num_batches = (num_artworks + batch_size - 1) // batch_size\n","\n","# 批量处理\n","for i in range(num_batches):\n","    start_idx = i * batch_size\n","    end_idx = min((i + 1) * batch_size, num_artworks)\n","\n","    # 获取当前批次的艺术画作和音乐评论\n","    artwork_batch = artwork_data['utterance'][start_idx:end_idx]\n","    music_batch = audio_data['caption']\n","\n","    # 文本编码\n","    artwork_tokens = tokenizer(list(artwork_batch), padding=True, truncation=True, return_tensors='pt')\n","    music_tokens = tokenizer(list(music_batch), padding=True, truncation=True, return_tensors='pt')\n","\n","    # 计算文本的BERT编码\n","    with torch.no_grad():\n","        artwork_outputs = model(**artwork_tokens)\n","        music_outputs = model(**music_tokens)\n","\n","    # 取BERT模型的最后一层的CLS token作为文本编码\n","    artwork_embeddings = artwork_outputs.last_hidden_state[:, 0, :].numpy()\n","    music_embeddings = music_outputs.last_hidden_state[:, 0, :].numpy()\n","\n","    # 计算相似度矩阵\n","    similarity_matrix = cosine_similarity(artwork_embeddings, music_embeddings)\n","\n","    # 获取每幅艺术画作与音乐之间的最匹配评论的索引和相似度得分\n","    best_match_indices = similarity_matrix.argmax(axis=1)\n","    best_match_scores = similarity_matrix.max(axis=1)\n","\n","    # 将匹配结果添加到匹配数据列表中\n","    for j, (artwork_index, score) in enumerate(zip(range(start_idx, end_idx), best_match_scores)):\n","        music_index = best_match_indices[j]\n","        artwork_name = artwork_data.loc[artwork_index, 'painting']\n","        artwork_text = artwork_data.loc[artwork_index, 'utterance']\n","        music_name = audio_data.loc[music_index, 'audio_name']\n","        music_comment = audio_data.loc[music_index, 'caption']\n","        matched_data.append([artwork_name, artwork_text, music_name, music_comment, score])\n","\n","# 将结果整理成新的DataFrame\n","matched_df = pd.DataFrame(matched_data, columns=['Artwork', 'Art_Utterance', 'Music_Name', 'Music_Comment', 'Similarity_Score'])\n","\n","# 将结果保存到新的CSV文件中\n","matched_df.to_csv(\"E:\\\\Project and Dissertation in Data Science\\\\dataset\\\\matched_data_TINYBERT.csv\", index=False)\n","\n","\n","# 计算模型指标\n","# 覆盖率：匹配的艺术画作评论和音乐评论的数量\n","total_artworks_matched = len(matched_df['Artwork'].unique())\n","total_music_matched = len(matched_df['Music_Name'].unique())\n","coverage_artwork = total_artworks_matched / len(artwork_data)\n","coverage_music = total_music_matched / len(audio_data)\n","coverage = (coverage_artwork + coverage_music) / 2\n","\n","# 多样性：匹配的音乐评论的唯一数量\n","diversity = total_music_matched / total_matches if total_matches > 0 else 0\n","\n","# 相似度的均值\n","similarity_mean = matched_df['Similarity_Score'].mean()\n","\n","# 打印模型指标\n","print(\"Model Metrics:\")\n","print(\"Coverage:\", coverage)\n","print(\"Diversity:\", diversity)\n","print(\"Similarity:\", similarity_mean)"]}]}